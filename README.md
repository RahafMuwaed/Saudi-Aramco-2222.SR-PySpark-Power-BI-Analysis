# Saudi Stock Analysis with PySpark & Power BI  
### ðŸ“ˆ Case Study: Aramco (Ticker: 2222.SR)

## 1. Project Overview

This project analyzes **Saudi Aramco stock (2222.SR)** using:

- **PySpark** for data preparation & analytics.
- **Power BI (web)** for building an interactive dashboard.

The goal is to give a **clean, professional** view of the stock performance over the last years, answering questions like:

- How did the stock price move over time?
- What are the **highest / lowest** prices in the whole period?
- What is the **average price** and **average trading volume**?
- How does price and volume change **by year**?

> **Dataset period:** from **2019-12-12** to **2025-12-02**  
> (This is the range in the `2222.SR.csv` file used in this project.)

---

## 2. Data

### 2.1 Source

- The main dataset is a CSV file downloaded from a **public financial data website** (e.g. Tadawul/Yahoo Finance).
- File name used in the notebook:  
  `data/raw/2222.SR.csv`

### 2.2 Main Columns (Raw Data)

Typical columns in the file:

- `Date` â€“ Trading day (e.g. `2024-01-15`).
- `Open` â€“ Price at market open.
- `High` â€“ Highest price during the day.
- `Low` â€“ Lowest price during the day.
- `Close` â€“ Closing price of the day.
- `Adj Close` â€“ Adjusted closing price (after dividends/splits if available).
- `Volume` â€“ Number of traded shares in that day.

These raw columns are the base for all calculations in PySpark and Power BI.

---

## 3. Tools & Technologies

- **OS:** macOS  
- **Environment:** Anaconda / Jupyter Notebook  
- **Language:** Python 3 + **PySpark**
- **Visualization:** **Power BI Service (web)**

---

## 4. Project Structure

```text
saudi-stocks-spark/
â”œâ”€ data/
â”‚  â””â”€ raw/
â”‚     â””â”€ 2222.SR.csv
â”œâ”€ notebooks/
â”‚  â””â”€ PySpark.ipynb
â””â”€ README.md
powerbi/
â””â”€ 2222_Aramco_Report.pbix
```

---

## 5. PySpark Analysis

All analysis is done in:  
`notebooks/PySpark.ipynb`

Below is a summary of the main steps and what each part does.

### 5.1 Loading the Data

```python
df_prices = (
    spark.read
        .option("header", True)
        .option("inferSchema", True)
        .csv("data/raw/2222.SR.csv")
)
```

### 5.2 Cleaning & Selecting Columns

We keep only the relevant columns:
	â€¢	Date
	â€¢	Open, High, Low, Close
	â€¢	Volume

We also ensure that:
	â€¢	Date is converted to a proper date type (if needed).
	â€¢	Numeric columns are correctly inferred as numbers (not strings).

### 5.3 Daily Returns

We calculate daily percentage return based on the closing price:

# (Close_today / Close_yesterday - 1) * 100

Implementation idea:
	â€¢	Use a Window function ordered by Date to get prev_close.
	â€¢	Create a new column: daily_return_pct.

Result: DataFrame df_ret_filtered with around 1,486 rows
(the first day has no previous close, so itâ€™s excluded from returns).

This allows us to analyze:
	â€¢	Best and worst daily performance.
	â€¢	Volatility patterns.

### 5.4 Monthly Statistics

We aggregate by Year-Month to get:
	â€¢	Average close per month.
	â€¢	Minimum & maximum close per month.
	â€¢	Average daily return per month.
	â€¢	Total monthly volume.

Result: DataFrame monthly_stats with 73 rows (each row = one month).

We also calculate monthly_volume separately if needed.


### 5.5 Yearly Averages

We aggregate by Year to get:
	â€¢	yearly_avg_close â€“ average closing price per year.
	â€¢	yearly_avg_volume â€“ average daily volume per year.

Result: DataFrame yearly_avg with 7 rows (one per year in the data).


### 5.6 Yearly Returns

We compute Year Return %:

# {Year Return %} = \left( \frac{\text{Last Close in year}}{\text{First Close in year}} - 1 \right) \times 100


Steps:
	1.	For each year:
	â€¢	Get first trading day close.
	â€¢	Get last trading day close.
	2.	Apply the formula and create year_return_pct.

Result: DataFrame year_returns with 7 rows (Year + Year Return %).


### 5.7 Max Drawdown Per Year

Max Drawdown = the worst (largest) drop from a peak to a bottom during the year.

For each year:
	â€¢	Track running maximum of Close.
	â€¢	Compute drawdown:
	
# drawdown = (Close / running_max - 1) * 100

â€¢Take the minimum drawdown (most negative) as max_drawdown_pct.

Result: DataFrame max_drawdown with 7 rows (Year + Max Drawdown %).




